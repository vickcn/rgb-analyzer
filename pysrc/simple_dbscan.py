#!/usr/bin/env python3
"""
ç°¡åŒ–ç‰ˆ DBSCAN åˆ†æè…³æœ¬
ä½¿ç”¨æ¨™è¨˜æ¨£æœ¬ï¼ˆåœ–ç‰‡åç¨±ï¼‰é€²è¡Œèšé¡é©—è­‰
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import adjusted_rand_score, silhouette_score
import re

def main():
    """ä¸»ç¨‹å¼"""
    # æª”æ¡ˆè·¯å¾‘ï¼ˆä½¿ç”¨å·²æ¸…ç†å–®ä½å¾Œçš„æª”æ¡ˆï¼‰
    excel_path = Path("output/color_analysis_result_numeric.xlsx")
    
    if not excel_path.exists():
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {excel_path}")
        print(f"ğŸ’¡ è«‹å…ˆåŸ·è¡Œ clean_excel_columns.py ä¾†æ¸…ç†å–®ä½")
        return
    
    # è®€å–æ•¸æ“š
    print("ğŸ“‚ è¼‰å…¥æ•¸æ“š...")
    df = pd.read_excel(excel_path)
    print(f"âœ… è¼‰å…¥ {len(df)} ç­†æ•¸æ“š")
    
    # å¾åœ–ç‰‡åç¨±æå–çœŸå¯¦æ¨™ç±¤
    true_labels = []
    for filename in df["åœ–ç‰‡åç¨±"]:
        # ç§»é™¤å‰¯æª”åå’Œæ•¸å­—å¾Œç¶´
        clean_name = re.sub(r'\d+$', '', filename.replace('.png', ''))
        true_labels.append(clean_name)
    
    print(f"ğŸ·ï¸  çœŸå¯¦é¡åˆ¥: {set(true_labels)}")
    
    # é¸æ“‡ç‰¹å¾µæ¬„ä½
    # å¯é¸æ“‡çš„ç‰¹å¾µçµ„åˆ
    feature_sets = {
        "default": ["R", "G", "B", "H (è‰²ç›¸)", "S (é£½å’Œåº¦)", "V (æ˜åº¦)", "è‰²æº« (K)"],
        "rgb_only": ["R", "G", "B"],
        "hsv_only": ["H (è‰²ç›¸)", "S (é£½å’Œåº¦)", "V (æ˜åº¦)"],
        "color_temp_focused": ["è‰²æº« (K)", "S (é£½å’Œåº¦)", "V (æ˜åº¦)"],
        "comprehensive": ["R", "G", "B", "H (è‰²ç›¸)", "S (é£½å’Œåº¦)", "V (æ˜åº¦)", "è‰²æº« (K)", "HSL_H", "HSL_S", "HSL_L"]
    }
    
    # ä½¿ç”¨é è¨­çµ„åˆ
    feature_columns = feature_sets["default"]
    print(f"ğŸ¯ ä½¿ç”¨ç‰¹å¾µ ({len(feature_columns)} å€‹):")
    for i, col in enumerate(feature_columns, 1):
        print(f"   {i}. {col}")
    
    # å¦‚æœç”¨æˆ¶æƒ³ä½¿ç”¨å…¶ä»–çµ„åˆï¼Œå¯ä»¥å–æ¶ˆè¨»è§£ä»¥ä¸‹ä»£ç¢¼
    # import sys
    # if len(sys.argv) > 1:
    #     choice = sys.argv[1]
    #     if choice in feature_sets:
    #         feature_columns = feature_sets[choice]
    #         print(f"æ”¹ç”¨çµ„åˆ: {choice}")
    
    # æº–å‚™æ•¸æ“š
    X = df[feature_columns].fillna(df[feature_columns].mean())
    
    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # DBSCAN åƒæ•¸æœç´¢
    print("ğŸ” å°‹æ‰¾æœ€ä½³åƒæ•¸...")
    best_score = -1
    best_params = None
    
    for eps in np.arange(0.5, 2.0, 0.1):
        for min_samples in range(3, 8):
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            labels = dbscan.fit_predict(X_scaled)
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            
            if n_clusters > 1:
                try:
                    ari = adjusted_rand_score(true_labels, labels)
                    silhouette = silhouette_score(X_scaled, labels)
                    score = 0.7 * ari + 0.3 * silhouette
                    
                    if score > best_score:
                        best_score = score
                        best_params = {
                            'eps': eps,
                            'min_samples': min_samples,
                            'labels': labels,
                            'ari': ari,
                            'silhouette': silhouette,
                            'n_clusters': n_clusters
                        }
                except:
                    continue
    
    if best_params is None:
        print("âŒ ç„¡æ³•æ‰¾åˆ°åˆé©åƒæ•¸")
        return
    
    # è¼¸å‡ºçµæœ
    print(f"\nâœ… æœ€ä½³åƒæ•¸:")
    print(f"   eps: {best_params['eps']:.2f}")
    print(f"   min_samples: {best_params['min_samples']}")
    print(f"   ç¾¤é›†æ•¸: {best_params['n_clusters']}")
    print(f"   ARI: {best_params['ari']:.3f}")
    print(f"   Silhouette: {best_params['silhouette']:.3f}")
    
    # åˆ†æçµæœ
    df_result = df.copy()
    df_result['çœŸå¯¦æ¨™ç±¤'] = true_labels
    df_result['èšé¡æ¨™ç±¤'] = best_params['labels']
    
    print(f"\nğŸ“Š èšé¡çµæœ:")
    confusion = pd.crosstab(df_result['çœŸå¯¦æ¨™ç±¤'], df_result['èšé¡æ¨™ç±¤'], margins=True)
    print(confusion)
    
    # å„ç¾¤é›†ç‰¹å¾µçµ±è¨ˆ
    print(f"\nğŸ¯ å„ç¾¤é›†ç‰¹å¾µå¹³å‡å€¼:")
    cluster_stats = df_result.groupby('èšé¡æ¨™ç±¤')[feature_columns].mean().round(1)
    print(cluster_stats)
    
    # å„²å­˜çµæœ
    output_dir = Path("output/dbscan_simple")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    df_result.to_csv(output_dir / "dbscan_results.csv", index=False, encoding='utf-8-sig')
    
    # å„²å­˜åƒæ•¸å’Œè©•ä¼°çµæœ
    with open(output_dir / "best_params.json", 'w', encoding='utf-8') as f:
        json.dump({
            'eps': best_params['eps'],
            'min_samples': best_params['min_samples'],
            'n_clusters': best_params['n_clusters'],
            'ari_score': best_params['ari'],
            'silhouette_score': best_params['silhouette'],
            'feature_columns': feature_columns
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_dir}")
    print("ğŸ“ è¼¸å‡ºæª”æ¡ˆ:")
    print("   â€¢ dbscan_results.csv - å®Œæ•´çµæœ")
    print("   â€¢ best_params.json - æœ€ä½³åƒæ•¸")

if __name__ == "__main__":
    main()
