#!/usr/bin/env python3
"""
ä½¿ç”¨ DBSCAN å°è‰²å…‰æ•¸æ“šé€²è¡Œèšé¡åˆ†æ
åˆ©ç”¨åœ–ç‰‡åç¨±ä½œç‚ºæ¨™è¨˜æ¨£æœ¬é€²è¡Œé©—è­‰
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import adjusted_rand_score, silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

def load_data_and_headers(excel_path, headers_json_path):
    """è¼‰å…¥æ•¸æ“šå’Œ header è³‡è¨Š"""
    # è®€å– Excel æ•¸æ“š
    df = pd.read_excel(excel_path)
    
    # è®€å– header è³‡è¨Š
    with open(headers_json_path, 'r', encoding='utf-8') as f:
        headers_info = json.load(f)
    
    return df, headers_info

def extract_true_labels_from_filename(df, filename_col="åœ–ç‰‡åç¨±"):
    """å¾åœ–ç‰‡åç¨±æå–çœŸå¯¦æ¨™ç±¤"""
    true_labels = []
    
    for filename in df[filename_col]:
        # ç§»é™¤å‰¯æª”åå’Œæ•¸å­—å¾Œç¶´
        base_name = filename.replace('.png', '').replace('.jpg', '')
        
        # ç§»é™¤æ•¸å­—å¾Œç¶´ï¼ˆå¦‚ æš–ç™½1, æš–ç™½2ï¼‰
        import re
        clean_name = re.sub(r'\d+$', '', base_name)
        
        true_labels.append(clean_name)
    
    return true_labels

def select_features_interactive(headers_info):
    """äº’å‹•å¼é¸æ“‡ç‰¹å¾µæ¬„ä½"""
    print("ğŸ” å¯ç”¨çš„ç‰¹å¾µæ¬„ä½:")
    suggested = headers_info.get("suggested_features_for_dbscan", headers_info.get("suggested_features_for_kmeans", []))
    
    for i, header in enumerate(headers_info["headers"]):
        marker = "âœ“" if header in suggested else " "
        print(f"  {i:2d}: [{marker}] {header}")
    
    print(f"\nğŸ’¡ å»ºè­°çš„ç‰¹å¾µæ¬„ä½ï¼ˆé©åˆ DBSCANï¼‰:")
    for feature in suggested:
        print(f"   â€¢ {feature}")
    
    # è®“ç”¨æˆ¶é¸æ“‡
    print(f"\nè«‹é¸æ“‡ç‰¹å¾µæ¬„ä½:")
    print(f"1. ä½¿ç”¨å»ºè­°çš„ç‰¹å¾µæ¬„ä½")
    print(f"2. è‡ªå®šç¾©é¸æ“‡")
    
    choice = input("é¸æ“‡ (1/2): ").strip()
    
    if choice == "1":
        return suggested
    else:
        print("è«‹è¼¸å…¥æ¬„ä½ç´¢å¼•ï¼Œç”¨é€—è™Ÿåˆ†éš” (ä¾‹: 3,4,5,7,8,9):")
        indices = input().strip().split(',')
        try:
            selected_features = [headers_info["headers"][int(i.strip())] for i in indices]
            return selected_features
        except:
            print("è¼¸å…¥æ ¼å¼éŒ¯èª¤ï¼Œä½¿ç”¨å»ºè­°ç‰¹å¾µ")
            return suggested

def perform_dbscan_analysis(df, feature_columns, true_labels, eps_range=None, min_samples_range=None):
    """åŸ·è¡Œ DBSCAN åˆ†æ"""
    # æº–å‚™ç‰¹å¾µæ•¸æ“š
    X = df[feature_columns].copy()
    
    # è™•ç†ç¼ºå¤±å€¼
    X = X.fillna(X.mean())
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åƒæ•¸ç¯„åœ
    if eps_range is None:
        eps_range = np.arange(0.3, 2.0, 0.1)
    if min_samples_range is None:
        min_samples_range = range(2, 10)
    
    best_params = None
    best_score = -1
    results = []
    
    print("ğŸ” å°‹æ‰¾æœ€ä½³ DBSCAN åƒæ•¸...")
    
    for eps in eps_range:
        for min_samples in min_samples_range:
            # åŸ·è¡Œ DBSCAN
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = dbscan.fit_predict(X_scaled)
            
            # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            
            if n_clusters > 1:  # è‡³å°‘è¦æœ‰2å€‹ç¾¤é›†æ‰èƒ½è¨ˆç®— silhouette score
                try:
                    silhouette = silhouette_score(X_scaled, cluster_labels)
                    ari = adjusted_rand_score(true_labels, cluster_labels)
                    
                    # ç¶œåˆè©•åˆ†ï¼ˆARI æ¬Šé‡è¼ƒé«˜ï¼Œå› ç‚ºæœ‰çœŸå¯¦æ¨™ç±¤ï¼‰
                    combined_score = 0.7 * ari + 0.3 * silhouette
                    
                    results.append({
                        'eps': eps,
                        'min_samples': min_samples,
                        'n_clusters': n_clusters,
                        'n_noise': n_noise,
                        'silhouette': silhouette,
                        'ari': ari,
                        'combined_score': combined_score,
                        'cluster_labels': cluster_labels
                    })
                    
                    if combined_score > best_score:
                        best_score = combined_score
                        best_params = {
                            'eps': eps,
                            'min_samples': min_samples,
                            'cluster_labels': cluster_labels
                        }
                        
                except:
                    continue
    
    return best_params, results, X_scaled, scaler

def analyze_clusters(df, cluster_labels, true_labels, feature_columns):
    """åˆ†æèšé¡çµæœ"""
    # æ·»åŠ èšé¡æ¨™ç±¤åˆ°æ•¸æ“šæ¡†
    df_analysis = df.copy()
    df_analysis['Cluster'] = cluster_labels
    df_analysis['True_Label'] = true_labels
    
    print("ğŸ“Š èšé¡çµæœåˆ†æ:")
    print(f"ç¸½ç¾¤é›†æ•¸: {len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)}")
    print(f"å™ªéŸ³é»æ•¸: {list(cluster_labels).count(-1)}")
    
    # ç¾¤é›†çµ±è¨ˆ
    cluster_stats = df_analysis.groupby('Cluster').agg({
        'True_Label': ['count', lambda x: Counter(x).most_common(1)[0]],
        **{col: 'mean' for col in feature_columns}
    }).round(2)
    
    print("\nğŸ¯ å„ç¾¤é›†ç‰¹å¾µ:")
    print(cluster_stats)
    
    # çœŸå¯¦æ¨™ç±¤ vs èšé¡æ¨™ç±¤å°ç…§è¡¨
    print("\nğŸ” çœŸå¯¦æ¨™ç±¤ vs èšé¡æ¨™ç±¤:")
    confusion_matrix = pd.crosstab(df_analysis['True_Label'], df_analysis['Cluster'], margins=True)
    print(confusion_matrix)
    
    return df_analysis, cluster_stats

def visualize_results(df_analysis, feature_columns, output_dir):
    """è¦–è¦ºåŒ–çµæœ"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¨­å®šä¸­æ–‡å­—é«”
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # 1. èšé¡çµæœæ•£é»åœ–ï¼ˆä½¿ç”¨å‰å…©å€‹ä¸»è¦ç‰¹å¾µï¼‰
    if len(feature_columns) >= 2:
        plt.figure(figsize=(12, 5))
        
        # çœŸå¯¦æ¨™ç±¤
        plt.subplot(1, 2, 1)
        for label in df_analysis['True_Label'].unique():
            mask = df_analysis['True_Label'] == label
            plt.scatter(df_analysis[mask][feature_columns[0]], 
                       df_analysis[mask][feature_columns[1]], 
                       label=label, alpha=0.7)
        plt.xlabel(feature_columns[0])
        plt.ylabel(feature_columns[1])
        plt.title('çœŸå¯¦æ¨™ç±¤åˆ†å¸ƒ')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # èšé¡çµæœ
        plt.subplot(1, 2, 2)
        for cluster in df_analysis['Cluster'].unique():
            mask = df_analysis['Cluster'] == cluster
            label = f'Cluster {cluster}' if cluster != -1 else 'Noise'
            plt.scatter(df_analysis[mask][feature_columns[0]], 
                       df_analysis[mask][feature_columns[1]], 
                       label=label, alpha=0.7)
        plt.xlabel(feature_columns[0])
        plt.ylabel(feature_columns[1])
        plt.title('DBSCAN èšé¡çµæœ')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'dbscan_clusters.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    # 2. ç‰¹å¾µåˆ†å¸ƒç†±åœ–
    plt.figure(figsize=(10, 8))
    cluster_means = df_analysis.groupby('Cluster')[feature_columns].mean()
    sns.heatmap(cluster_means.T, annot=True, cmap='viridis', fmt='.1f')
    plt.title('å„ç¾¤é›†ç‰¹å¾µå¹³å‡å€¼ç†±åœ–')
    plt.tight_layout()
    plt.savefig(output_dir / 'cluster_features_heatmap.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ä¸»ç¨‹å¼"""
    # æª”æ¡ˆè·¯å¾‘
    base_dir = Path(__file__).parent
    excel_path = base_dir / "output" / "color_analysis_result.xlsx"
    headers_json_path = base_dir / "output" / "color_analysis_result_headers.json"
    output_dir = base_dir / "output" / "dbscan_analysis"
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not excel_path.exists():
        print(f"âŒ Excel æª”æ¡ˆä¸å­˜åœ¨: {excel_path}")
        return
    
    if not headers_json_path.exists():
        print(f"âŒ Headers JSON ä¸å­˜åœ¨: {headers_json_path}")
        print("è«‹å…ˆåŸ·è¡Œ extract_headers.py")
        return
    
    # è¼‰å…¥æ•¸æ“š
    print("ğŸ“‚ è¼‰å…¥æ•¸æ“š...")
    df, headers_info = load_data_and_headers(excel_path, headers_json_path)
    
    # æå–çœŸå¯¦æ¨™ç±¤
    true_labels = extract_true_labels_from_filename(df)
    print(f"âœ… è¼‰å…¥ {len(df)} ç­†æ•¸æ“šï¼Œ{len(set(true_labels))} å€‹çœŸå¯¦é¡åˆ¥")
    
    # é¸æ“‡ç‰¹å¾µ
    feature_columns = select_features_interactive(headers_info)
    print(f"ğŸ¯ é¸æ“‡çš„ç‰¹å¾µ: {feature_columns}")
    
    # åŸ·è¡Œ DBSCAN åˆ†æ
    print("ğŸ¤– åŸ·è¡Œ DBSCAN åˆ†æ...")
    best_params, results, X_scaled, scaler = perform_dbscan_analysis(
        df, feature_columns, true_labels
    )
    
    if best_params is None:
        print("âŒ ç„¡æ³•æ‰¾åˆ°åˆé©çš„ DBSCAN åƒæ•¸")
        return
    
    print(f"âœ… æœ€ä½³åƒæ•¸: eps={best_params['eps']:.2f}, min_samples={best_params['min_samples']}")
    
    # åˆ†æèšé¡çµæœ
    df_analysis, cluster_stats = analyze_clusters(
        df, best_params['cluster_labels'], true_labels, feature_columns
    )
    
    # è¦–è¦ºåŒ–çµæœ
    print("ğŸ“Š ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
    visualize_results(df_analysis, feature_columns, output_dir)
    
    # å„²å­˜çµæœ
    df_analysis.to_csv(output_dir / "dbscan_results.csv", index=False, encoding='utf-8-sig')
    
    # å„²å­˜åƒæ•¸èª¿å„ªçµæœ
    results_df = pd.DataFrame(results)
    results_df.to_csv(output_dir / "parameter_tuning_results.csv", index=False)
    
    print(f"ğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_dir}")
    print("ğŸ“ è¼¸å‡ºæª”æ¡ˆ:")
    print("   â€¢ dbscan_results.csv - å®Œæ•´åˆ†æçµæœ")
    print("   â€¢ parameter_tuning_results.csv - åƒæ•¸èª¿å„ªçµæœ")
    print("   â€¢ dbscan_clusters.png - èšé¡è¦–è¦ºåŒ–")
    print("   â€¢ cluster_features_heatmap.png - ç‰¹å¾µç†±åœ–")

if __name__ == "__main__":
    main()
